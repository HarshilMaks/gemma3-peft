{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ghost Architect \u2014 Colab T4 Main Notebook\n",
        "\n",
        "This is the single notebook to run full **Gemma-3 Trinity training** on Google Colab T4 (16GB).\n",
        "\n",
        "## What this notebook does\n",
        "1. Validates T4 runtime\n",
        "2. Installs exact dependencies\n",
        "3. Syncs project files\n",
        "4. Writes full T4 training configuration\n",
        "5. Launches training\n",
        "6. Exports GGUF artifacts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Runtime Check (must be T4 GPU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "assert torch.cuda.is_available(), 'CUDA is not available. Set Runtime > GPU in Colab.'\n",
        "gpu_name = torch.cuda.get_device_name(0)\n",
        "gpu_mem_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "print(f'GPU: {gpu_name}')\n",
        "print(f'VRAM: {gpu_mem_gb:.1f} GB')\n",
        "if 'T4' not in gpu_name:\n",
        "    print('Warning: This notebook is tuned for T4; adjust config if using a different GPU.')\n",
        "\n",
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q unsloth[colab-new]==2026.1.4\n",
        "!pip install -q \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes\n",
        "!pip install -q \"torch>=2.1.0\" \"transformers>=4.38.0\" datasets numpy scipy tqdm\n",
        "\n",
        "print('Dependencies installed.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Mount Drive and Sync Repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Option A: clone from GitHub\n",
        "# REPO_URL = 'https://github.com/<your-org>/<your-repo>.git'\n",
        "# !git clone $REPO_URL /content/ghost_architect_gemma3\n",
        "\n",
        "# Option B: copy from Drive (recommended if already uploaded)\n",
        "# !cp -r /content/drive/MyDrive/ghost_architect_gemma3 /content/\n",
        "\n",
        "os.makedirs('/content/ghost_architect_gemma3', exist_ok=True)\n",
        "%cd /content/ghost_architect_gemma3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Full T4 Trinity Training Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config_yaml = '''\n",
        "model_name: \"unsloth/gemma-3-12b-it-bnb-4bit\"\n",
        "max_seq_length: 4096\n",
        "load_in_4bit: true\n",
        "\n",
        "lora:\n",
        "  r: 64\n",
        "  lora_alpha: 32\n",
        "  target_modules:\n",
        "    - q_proj\n",
        "    - k_proj\n",
        "    - v_proj\n",
        "    - o_proj\n",
        "    - gate_proj\n",
        "    - up_proj\n",
        "    - down_proj\n",
        "  use_rslora: true\n",
        "  use_dora: true\n",
        "  lora_dropout: 0.1\n",
        "\n",
        "training:\n",
        "  per_device_train_batch_size: 1\n",
        "  gradient_accumulation_steps: 4\n",
        "  learning_rate: 2e-4\n",
        "  max_steps: 60\n",
        "  warmup_steps: 10\n",
        "  logging_steps: 1\n",
        "  save_steps: 20\n",
        "  optimizer: \"adamw_8bit\"\n",
        "  lr_scheduler_type: \"cosine\"\n",
        "\n",
        "output:\n",
        "  adapters_dir: \"output/adapters\"\n",
        "  checkpoints_dir: \"output/checkpoints\"\n",
        "  gguf_dir: \"output/gguf\"\n",
        "\n",
        "oom_fallbacks:\n",
        "  - {action: reduce_seq_len, value: 2048}\n",
        "  - {action: reduce_rank, value: 32}\n",
        "  - {action: disable_dora, value: false}\n",
        "'''\n",
        "\n",
        "os.makedirs('configs', exist_ok=True)\n",
        "with open('configs/training_config_colab_t4.yaml', 'w') as f:\n",
        "    f.write(config_yaml)\n",
        "\n",
        "print('Wrote configs/training_config_colab_t4.yaml')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Validate Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, json\n",
        "\n",
        "dataset_path = 'data/dataset.json'\n",
        "assert os.path.exists(dataset_path), f'Missing dataset: {dataset_path}'\n",
        "\n",
        "with open(dataset_path, 'r') as f:\n",
        "    raw = f.read().strip()\n",
        "\n",
        "if raw:\n",
        "    _ = json.loads(raw)\n",
        "    print('Dataset JSON is valid.')\n",
        "else:\n",
        "    print('Dataset file is empty. Populate data/dataset.json before training.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Launch Full Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Expected CLI in future implementation:\n",
        "# !python src/train.py --config configs/training_config_colab_t4.yaml --dataset data/dataset.json\n",
        "\n",
        "print('Implement src/train.py next, then run the command above.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Export to GGUF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Expected CLI in future implementation:\n",
        "# !python src/export.py --adapter_dir output/adapters --output_dir output/gguf --quantization q4_k_m\n",
        "\n",
        "print('Implement src/export.py next, then run the command above.')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}